{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Warning: To use the exr data format, please install the OpenEXR package following the instructions detailed in the README at github.com/tensorflow/graphics.\n",
      "Warning: To use the threejs_vizualization, please install the colabtools package following the instructions detailed in the README at github.com/tensorflow/graphics.\n",
      "WARNING: Logging before flag parsing goes to stderr.\nW1101 13:49:16.419544 17624 deprecation.py:323] From F:\\tf_projects\\3D\\FeaStNet-master\\common\\custom_layer.py:22: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `layer.add_weight` method instead.\n",
      "W1101 13:49:16.428493 17624 deprecation.py:506] From d:\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-22cff47b9c01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mckpt_full_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mckpt_full_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mckpt_full_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, save_path)\u001b[0m\n\u001b[0;32m   1969\u001b[0m           \u001b[1;33m(\u001b[0m\u001b[0moften\u001b[0m \u001b[0mat\u001b[0m \u001b[0mprogram\u001b[0m \u001b[0mshutdown\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1970\u001b[0m     \"\"\"\n\u001b[1;32m-> 1971\u001b[1;33m     \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1972\u001b[0m     \u001b[1;31m# Create the save counter now so it gets initialized with other variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1973\u001b[0m     \u001b[1;31m# when graph building. Creating it earlier would lead to double\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, save_path)\u001b[0m\n\u001b[0;32m   1226\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msave_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1227\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mInitializationOnlyStatus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_view\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1228\u001b[1;33m     \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1229\u001b[0m     \u001b[0mgraph_building\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1230\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgraph_building\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[1;34m(filepattern)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mNewCheckpointReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 873\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[0mNewCheckpointReader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tf_api_names_v1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'train.NewCheckpointReader'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mthis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_CheckpointReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to get matching files on ../ckpt\\20190823-1511/rutine\\ckpt-80: Not found: FindFirstFile failed for: ../ckpt/20190823-1511/rutine : The system cannot find the path specified.\r\n; No such process"
     ],
     "ename": "InvalidArgumentError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to get matching files on ../ckpt\\20190823-1511/rutine\\ckpt-80: Not found: FindFirstFile failed for: ../ckpt/20190823-1511/rutine : The system cannot find the path specified.\r\n; No such process",
     "output_type": "error"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import trimesh\n",
    "import tensorboard\n",
    "from common.extract_model import ExtractModel\n",
    "from common.freeze_wrapper import write_pb\n",
    "from feature_detect.src.config import *\n",
    "from feature_detect.src.feat_data_keras import Data_Gen, Rotate_feed\n",
    "\n",
    "from tensorboard.plugins.mesh import summary as mesh_summary\n",
    "from common.plc import build_plc, build_feed_dict\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "npz_path='F:/ProjectData/mesh_feature/test/test_npz/front'\n",
    "log_dir = 'F:/ProjectData/mesh_feature/test/log_dir'\n",
    "batch_size = 1\n",
    "\n",
    "plc,input_names=build_plc(BLOCK_NUM,need_batch=True)\n",
    "model=ExtractModel(CHANNELS,coarse_level=C_LEVEL,fc_dim=4)\n",
    "load_time_dir = '20190823-1511/rutine'  # where to restore the model\n",
    "ckpt_file = 'ckpt-80'\n",
    "output = model(plc,need_sqeeze=True)\n",
    "output=tf.identity(output,'output_node')\n",
    "ckpt_full_dir = os.path.join(CKPT_PATH, load_time_dir)\n",
    "ckpt_full_path = os.path.join(ckpt_full_dir, ckpt_file)\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "status = checkpoint.restore(ckpt_full_path)\n",
    "\n",
    "\n",
    "need_freeze=True\n",
    "need_infer=True\n",
    "need_summary=True\n",
    "\n",
    "version=1\n",
    "model_path=os.path.join('../freeze_output',str(version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3d91643eba52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_or_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;31m# for node in tf.train.list_variables(tf.train.latest_checkpoint(ckpt_full_dir)):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#     print(node)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mneed_freeze\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'status' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'status' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    status.initialize_or_restore(sess)\n",
    "    # for node in tf.train.list_variables(tf.train.latest_checkpoint(ckpt_full_dir)):\n",
    "    #     print(node)\n",
    "    if need_freeze:\n",
    "        if os.path.exists(model_path):\n",
    "            import shutil\n",
    "            shutil.rmtree(model_path)\n",
    "        else:\n",
    "            os.mkdir(model_path)\n",
    "    \n",
    "        def build_input_info(plc_dict):\n",
    "            tensor_infos={'vertice':plc['vertice']}\n",
    "            adj_infos={'adj_%d' % i:adj_plc for i,adj_plc in enumerate(plc['adjs'])}\n",
    "            perm_infos={'perm_%d' % i:perm_plc for i,perm_plc in enumerate(plc['perms'])}\n",
    "            tensor_infos.update(adj_infos)\n",
    "            tensor_infos.update(perm_infos)\n",
    "            return tensor_infos\n",
    "        # ordinary model\n",
    "        tf.compat.v1.saved_model.simple_save(sess, model_path, build_input_info(plc), {'output_node': output})\n",
    "        write_pb(input_saved_model_dir=model_path,\n",
    "                 output_graph_filename=\"../output_graph.pb\")\n",
    "        \n",
    "    elif need_infer:\n",
    "        data_gen = Data_Gen('F:/ProjectData/mesh_feature/Case_npz/back')\n",
    "        rf = Rotate_feed(\n",
    "            rot_num=2,\n",
    "            rot_range=[np.pi / 18., np.pi / 18., np.pi / 18.],\n",
    "            angle_fixed=True,\n",
    "            data_gen=data_gen\n",
    "        )\n",
    "        epoch_end=False\n",
    "        \n",
    "    \n",
    "        while(not epoch_end):\n",
    "            feed_numpy,epoch_end = rf.rotate_case()\n",
    "            x=feed_numpy['vertice']\n",
    "            adjs=feed_numpy['adjs']\n",
    "            perms=feed_numpy['perms']\n",
    "            feed_dict=build_feed_dict(plc,x,adjs,perms)\n",
    "            result=sess.run(output,feed_dict=feed_dict)\n",
    "            print(result)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Camera and scene configuration.\n",
    "config_dict = {\n",
    "    'material': {\n",
    "      'cls': 'PointsMaterial',\n",
    "      'size': 1,\n",
    "        # 'color':0x000fff\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Read all sample PLY files."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "data_gen = Data_Gen(npz_path)\n",
    "data,  npz_name,epoch_end = data_gen.load_pkg()\n",
    "\n",
    "# vertices=data['x'].astype(np.float32)\n",
    "# features=data['y'].astype(np.float32)\n",
    "# \n",
    "# # Add batch dimension, so our data will be of shape BxNxC.\n",
    "# points_v = vertices\n",
    "# points_f=features[:,1:]\n",
    "\n",
    "vertices=data['x'][1].astype(np.float32)\n",
    "features=data['y'][1].astype(np.float32)\n",
    "\n",
    "# Add batch dimension, so our data will be of shape BxNxC.\n",
    "points_v = np.expand_dims(vertices, 0)\n",
    "points_f=np.expand_dims(features[:,1:],0)\n",
    "points=np.concatenate([points_v,points_f],axis=1)\n",
    "\n",
    "\n",
    "colors_v = np.ones_like(points_v)*[0,0,255]\n",
    "colors_f = np.ones_like(points_f)*[0,255,0]\n",
    "colors=np.concatenate([colors_v,colors_f],axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "points_tensor = tf.compat.v1.placeholder(tf.float32, points.shape)\n",
    "colors_tensor = tf.compat.v1.placeholder(tf.int32, colors.shape)\n",
    "\n",
    "summary = mesh_summary.op(\n",
    "    'v_color_tensor', \n",
    "    vertices=points_tensor,\n",
    "    colors=colors_tensor, \n",
    "    config_dict=config_dict\n",
    ")\n",
    "\n",
    "# Create summary writer and session.\n",
    "writer = tf.compat.v1.summary.FileWriter(log_dir)\n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "summaries = sess.run([summary], feed_dict={\n",
    "    points_tensor: points,\n",
    "    colors_tensor: colors,\n",
    "})\n",
    "# Save summaries.\n",
    "for summary in summaries:\n",
    "  writer.add_summary(summary)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}