{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Warning: To use the exr data format, please install the OpenEXR package following the instructions detailed in the README at github.com/tensorflow/graphics.\n",
      "Warning: To use the threejs_vizualization, please install the colabtools package following the instructions detailed in the README at github.com/tensorflow/graphics.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From D:\\py_Project\\feastnet\\common\\custom_layer.py:22: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From d:\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import trimesh\n",
    "import tensorboard\n",
    "from common.extract_model import ExtractModel\n",
    "from common.freeze_wrapper import write_pb\n",
    "from feature_detect.src.config import *\n",
    "from feature_detect.src.feat_data_keras import Data_Gen, Rotate_feed\n",
    "\n",
    "from tensorboard.plugins.mesh import summary as mesh_summary\n",
    "from common.plc import build_plc, build_feed_dict\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "npz_path='F:/ProjectData/mesh_feature/test/test_npz/front'\n",
    "log_dir = 'F:/ProjectData/mesh_feature/test/log_dir'\n",
    "batch_size = 1\n",
    "\n",
    "plc,input_names=build_plc(BLOCK_NUM,need_batch=True)\n",
    "model=ExtractModel(CHANNELS,coarse_level=C_LEVEL,fc_dim=4)\n",
    "load_time_dir = '20190823-1511/rutine'  # where to restore the model\n",
    "ckpt_file = 'ckpt-80'\n",
    "output = model(plc,need_sqeeze=True)\n",
    "output=tf.identity(output,'output_node')\n",
    "ckpt_full_dir = os.path.join(CKPT_PATH, load_time_dir)\n",
    "ckpt_full_path = os.path.join(ckpt_full_dir, ckpt_file)\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "status = checkpoint.restore(ckpt_full_path)\n",
    "\n",
    "\n",
    "need_freeze=True\n",
    "need_infer=True\n",
    "need_summary=True\n",
    "\n",
    "version=1\n",
    "model_path=os.path.join('../freeze_output',str(version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-cc25bbdc1586>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# with tf.compat.v1.Session() as sess:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_or_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# for node in tf.train.list_variables(tf.train.latest_checkpoint(ckpt_full_dir)):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#     print(node)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'status' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'status' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sess=tf.compat.v1.Session()\n",
    "# with tf.compat.v1.Session() as sess:\n",
    "status.initialize_or_restore(sess)\n",
    "# for node in tf.train.list_variables(tf.train.latest_checkpoint(ckpt_full_dir)):\n",
    "#     print(node)\n",
    "if need_freeze:\n",
    "    if os.path.exists(model_path):\n",
    "        import shutil\n",
    "        shutil.rmtree(model_path)\n",
    "    else:\n",
    "        os.mkdir(model_path)\n",
    "\n",
    "    def build_input_info(plc_dict):\n",
    "        tensor_infos={'vertice':plc['vertice']}\n",
    "        adj_infos={'adj_%d' % i:adj_plc for i,adj_plc in enumerate(plc['adjs'])}\n",
    "        perm_infos={'perm_%d' % i:perm_plc for i,perm_plc in enumerate(plc['perms'])}\n",
    "        tensor_infos.update(adj_infos)\n",
    "        tensor_infos.update(perm_infos)\n",
    "        return tensor_infos\n",
    "    # ordinary model\n",
    "    tf.compat.v1.saved_model.simple_save(sess, model_path, build_input_info(plc), {'output_node': output})\n",
    "    write_pb(input_saved_model_dir=model_path,\n",
    "             output_graph_filename=\"../output_graph.pb\")\n",
    "    \n",
    "elif need_infer:\n",
    "    data_gen = Data_Gen('F:/ProjectData/mesh_feature/Case_npz/back')\n",
    "    rf = Rotate_feed(\n",
    "        rot_num=2,\n",
    "        rot_range=[np.pi / 18., np.pi / 18., np.pi / 18.],\n",
    "        angle_fixed=True,\n",
    "        data_gen=data_gen\n",
    "    )\n",
    "    epoch_end=False\n",
    "    \n",
    "\n",
    "    while(not epoch_end):\n",
    "        feed_numpy,epoch_end = rf.rotate_case()\n",
    "        x=feed_numpy['vertice']\n",
    "        adjs=feed_numpy['adjs']\n",
    "        perms=feed_numpy['perms']\n",
    "        feed_dict=build_feed_dict(plc,x,adjs,perms)\n",
    "        result=sess.run(output,feed_dict=feed_dict)\n",
    "        print(result)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Camera and scene configuration.\n",
    "config_dict = {\n",
    "    'material': {\n",
    "      'cls': 'PointsMaterial',\n",
    "      'size': 1,\n",
    "        # 'color':0x000fff\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Read all sample PLY files."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "data_gen = Data_Gen(npz_path)\n",
    "data,  npz_name,epoch_end = data_gen.load_pkg()\n",
    "\n",
    "# vertices=data['x'].astype(np.float32)\n",
    "# features=data['y'].astype(np.float32)\n",
    "# \n",
    "# # Add batch dimension, so our data will be of shape BxNxC.\n",
    "# points_v = vertices\n",
    "# points_f=features[:,1:]\n",
    "\n",
    "vertices=data['x'][1].astype(np.float32)\n",
    "features=data['y'][1].astype(np.float32)\n",
    "\n",
    "# Add batch dimension, so our data will be of shape BxNxC.\n",
    "points_v = np.expand_dims(vertices, 0)\n",
    "points_f=np.expand_dims(features[:,1:],0)\n",
    "points=np.concatenate([points_v,points_f],axis=1)\n",
    "\n",
    "\n",
    "colors_v = np.ones_like(points_v)*[0,0,255]\n",
    "colors_f = np.ones_like(points_f)*[0,255,0]\n",
    "colors=np.concatenate([colors_v,colors_f],axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "points_tensor = tf.compat.v1.placeholder(tf.float32, points.shape)\n",
    "colors_tensor = tf.compat.v1.placeholder(tf.int32, colors.shape)\n",
    "\n",
    "summary = mesh_summary.op(\n",
    "    'v_color_tensor', \n",
    "    vertices=points_tensor,\n",
    "    colors=colors_tensor, \n",
    "    config_dict=config_dict\n",
    ")\n",
    "\n",
    "# Create summary writer and session.\n",
    "writer = tf.compat.v1.summary.FileWriter(log_dir)\n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "summaries = sess.run([summary], feed_dict={\n",
    "    points_tensor: points,\n",
    "    colors_tensor: colors,\n",
    "})\n",
    "# Save summaries.\n",
    "for summary in summaries:\n",
    "  writer.add_summary(summary)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}